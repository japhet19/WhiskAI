<instructions_for_debug_issue>
    ultrathink: Based on the `issue_description` from `action_args`, and any provided file paths or code snippets, formulate a `ComprehensiveDiagnosticStrategy`. This strategy object must contain: (a) `RootCauseHypotheses` (list of at least 3-5 plausible, distinct hypotheses, each with a unique ID and brief rationale), (b) `RankedHypotheses` (list of hypothesis IDs from (a), ranked by likelihood and test complexity), (c) `VerificationSteps` (dictionary mapping each hypothesis ID to an ordered list of specific, minimal diagnostic actions/questions to validate or invalidate it â€“ e.g., 'Check specific log for X', 'Inspect function Y in file Z for condition A', 'Ask user about recent change B'), (d) `EnvironmentalFactorsToConsider` (list of potential environmental elements that could affect the issue), (e) `InitialFixComplexityAssessment` (dictionary mapping likely hypothesis IDs to an estimated fix complexity [e.g., Low, Medium, High] vs. expected stability improvement). This `ComprehensiveDiagnosticStrategy` will be the primary guide for the entire interactive debugging process.
    0.  <operational_guidelines>Throughout this command: (a) If any step results in an unexpected error from a tool or internal logic, clearly report the error, the step where it occurred, and ask the user for guidance (e.g., retry, skip, provide more info, abort). (b) When prompting the user for input, always aim for clarity and specify what kind of information is most helpful for the current context or strategy.</operational_guidelines>
    1.  <role_adoption>Adopt the role: "You are an expert Debugging Specialist."</role_adoption>
    2.  <input_processing>Parse `issue_description`, and optionally file paths or code snippets, from `action_args`. If `issue_description` is empty, ask for details. If still no details, report "Error: Issue description required." and stop. This information is crucial for the `ultrathink` analysis.</input_processing>
    3.  <context_gathering>Read relevant memory files (e.g., `active_context.md`, `architecture.md`). If file paths were provided in `action_args`:
        a.  For each provided file path, first verify if it is accessible. If not, report and do not attempt to read.
        b.  For accessible paths, read the files. Analyze file contents, snippets, and logs.
        Goal: Gather all necessary context to enable `ultrathink` to generate a robust `ComprehensiveDiagnosticStrategy`. If context seems thin for some hypotheses, the `VerificationSteps` in the strategy should include questions to the user to enrich it.
        </context_gathering>
    4.  <diagnostic_process_execution>
        Instruct yourself: "I will now execute the `ComprehensiveDiagnosticStrategy` produced by my `ultrathink` analysis to diagnose the issue: `[issue description]`. I will maintain a `SessionState` (in-memory object) with `CurrentHypothesisID`, `TestedHypothesesIDs` (list), `EvidenceCollected` (string), `DiagnosticHistory` (log of actions/outcomes)."
        <iterative_diagnostic_steps>
          <step>Set `CurrentHypothesisID` to the next untested hypothesis from `ComprehensiveDiagnosticStrategy.RankedHypotheses` (skip those in `TestedHypothesesIDs`). If all ranked hypotheses are tested, and no root cause is found, instruct yourself: 'All initial hypotheses tested. Engaging "think hard" to deeply reflect on all `EvidenceCollected` and the original `ComprehensiveDiagnosticStrategy` to identify new potential avenues or to confirm if current information is insufficient.' Then, re-evaluate `EvidenceCollected` and `ComprehensiveDiagnosticStrategy` to generate a new `CurrentHypothesisID` or declare that a solution cannot be found with current info.</step>
          <step>Retrieve the ordered `VerificationSteps` for the `CurrentHypothesisID` from `ComprehensiveDiagnosticStrategy.VerificationSteps[CurrentHypothesisID]`.</step>
          <step>For the `CurrentHypothesisID`, propose the next diagnostic action from its `VerificationSteps`. Phrase this considering `ComprehensiveDiagnosticStrategy.EnvironmentalFactorsToConsider`. Example: "To test hypothesis '{ComprehensiveDiagnosticStrategy.RootCauseHypotheses[CurrentHypothesisID].rationale}', I propose to [action from VerificationSteps]."</step>
          <step>If proposing to view code, specify exact file path and line range or function/class name for `view_code_item` or `view_line_range`. If asking a question, be precise. If suggesting a command, provide it fully.</step>
          <step>Present action to user. If it's an info request, clearly specify what information is needed based on the current `VerificationStep` (e.g., 'Please provide the exact error message from the console log around [timestamp/event], as I'm testing hypothesis X related to Y'). Then, await response. If Claude can act (e.g., view code), ask: "Shall I proceed? (y/n)". Add action to `DiagnosticHistory`.</step>
          <step>Analyze user's response or tool output. Append to `EvidenceCollected` and `DiagnosticHistory`. Add `CurrentHypothesisID` to `TestedHypothesesIDs`.</step>
          <step>Evaluate if `CurrentHypothesisID` is supported, refuted, or needs more specific data based on new `EvidenceCollected`. Explicitly state: 'Re-evaluating `CurrentHypothesisID` against `ComprehensiveDiagnosticStrategy` and new `EvidenceCollected` to determine next best step.' Clearly state reasoning. If refuted, loop to next hypothesis. If strongly supported, proceed to identify root cause.</step>
          <step>Once root cause is likely identified (a hypothesis is strongly supported by `EvidenceCollected`), clearly explain: "The root cause appears to be: {ComprehensiveDiagnosticStrategy.RootCauseHypotheses[CurrentHypothesisID].rationale}, supported by: {summary of key evidence}."</step>
          <step>Before proposing a fix, instruct yourself: 'Reflect: Does this proposed fix directly address the validated root cause ({ComprehensiveDiagnosticStrategy.RootCauseHypotheses[CurrentHypothesisID].rationale}) and align with the `InitialFixComplexityAssessment`?' Then, propose a specific fix, referring to `ComprehensiveDiagnosticStrategy.InitialFixComplexityAssessment[CurrentHypothesisID]`. E.g., "I recommend [specific change] because [reason]. This is a [complexity] fix with an expected [stability_improvement] benefit."</step>
        </iterative_diagnostic_steps>
        </diagnostic_process_execution>
    5.  <resolution_proposal>After explaining the cause and proposing a fix, ask for confirmation: "Based on the diagnostics, the proposed fix is: `<proposed_fix_summary>...</proposed_fix_summary>`. Shall I attempt to apply this fix to the relevant file(s) using my file editing tool? (y/n)".</resolution_proposal>
    6.  <execution>If user confirms 'y' AND the fix involves code changes Claude Code can make directly, instruct your file editing tool to apply the changes. If the fix involves commands the user must run, guide them. Report success or failure.</execution>
    7.  <post_action>If the fix is applied:
        a.  Suggest updating relevant task statuses (e.g., if this debugging session was related to an existing task, its status might change or notes could be added). Propose specific changes to task files and confirm before acting.
        b.  If this was an unplanned debugging session and a new follow-up task should be created (e.g., to refactor related code, add more tests), propose this to the user. If confirmed, state: "I will now use the `create_task.md` command to create this follow-up task. What should the description be for this new task?" Then, upon receiving the description, invoke the logic of `create_task.md` (or guide the user to invoke `/create_task [description]`). Confirm task creation details before proceeding.
        </post_action>
</instructions_for_debug_issue>